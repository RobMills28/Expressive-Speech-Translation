# Dockerfile v13 - The "Flatten the Package" Fix
FROM --platform=linux/amd64 python:3.10-slim-bullseye

ENV PYTHONUNBUFFERED=1

# We no longer need any PYTHONPATH or complex WORKDIR/CMD logic.

# 1. System Dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git wget ffmpeg build-essential cmake libgl1-mesa-glx && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# 2. PyTorch & MMLab dependencies - GPU VERSION FOR AWS
WORKDIR /app
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118 && \
    pip install --no-cache-dir -U openmim && \
    mim install mmengine && \
    mim install "mmcv==2.0.1" && \
    pip install --no-cache-dir "xtcocotools>=1.12" && \
    mim install "mmdet==3.1.0" && \
    mim install "mmpose==1.1.0"

# 3. --- THE CRITICAL RESTRUCTURE ---
# Copy the entire MuseTalk project into a temporary location
COPY MuseTalk /app/MuseTalk_temp

# Create the final, clean project directory
RUN mkdir -p /app/MuseTalk

# Move the CORE contents of the 'musetalk' sub-package up to the main level.
# This makes 'utils', 'models', etc., top-level directories.
RUN mv /app/MuseTalk_temp/musetalk/* /app/MuseTalk/

# Move the top-level 'models' and other necessary files into our final directory.
RUN mv /app/MuseTalk_temp/models /app/MuseTalk/
RUN mv /app/MuseTalk_temp/requirements.txt /app/MuseTalk/
RUN mv /app/MuseTalk_temp/download_weights.sh /app/MuseTalk/

# Clean up the temporary directory
RUN rm -rf /app/MuseTalk_temp

# 4. Set the final, simple working directory
WORKDIR /app/MuseTalk

# 5. Install requirements from the new flattened location
RUN sed -i '/tensorflow/d' requirements.txt && \
    sed -i '/tensorboard/d' requirements.txt && \
    pip install --no-cache-dir --retries 5 --timeout 30 -r requirements.txt

# 6. Download Weights from the new flattened location
RUN chmod +x ./download_weights.sh && ./download_weights.sh

# 7. Manually download critical models
RUN mkdir -p ./models/face-parse-bisent/ && \
    mkdir -p ./models/musetalkV15/ && \
    pip install --no-cache-dir gdown && \
    gdown --id 154JgKpzCPW82qINcVieuPH3fZ2e0P812 -O ./models/face-parse-bisent/79999_iter.pth && \
    wget -O ./models/face-parse-bisent/resnet18-5c106cde.pth https://download.pytorch.org/models/resnet18-5c106cde.pth && \
    wget -O ./models/musetalkV15/unet.pth https://huggingface.co/TMElyralab/MuseTalk/resolve/main/musetalkV15/unet.pth && \
    pip uninstall -y gdown

# 8. Copy our API logic into the flattened project directory
COPY Docker/api_inference_logic.py /app/MuseTalk/
COPY Docker/musetalk_api.py /app/MuseTalk/
RUN pip install --no-cache-dir fastapi "uvicorn[standard]"

# 9. Run the API from the simple, flat directory
EXPOSE 8000
CMD ["uvicorn", "musetalk_api:app", "--host", "0.0.0.0", "--port", "8000"]