nohup: ignoring input
[2025-08-11 07:30:46,281] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/envs/pytorch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
2025-08-11 07:31:00,307 INFO training on multiple gpus, this gpu 0, rank 0, world_size 1
2025-08-11 07:31:19,465 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/init.pt
start step 13002 start epoch 7
2025-08-11 07:31:19,468 INFO Epoch 8 TRAIN info lr 1e-05 rank 0
2025-08-11 07:31:19,468 INFO using accumulate grad, new batch size is 1 times larger than before
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2025-08-11 07:31:37,716 WARNING get infinite grad_norm, check your code/data if it appears frequently
2025-08-11 07:32:28,721 DEBUG TRAIN Batch 8/100 loss 1.007096 acc 0.719222 lr 0.00001000 grad_norm 17.416300 rank 0
2025-08-11 07:33:22,044 DEBUG TRAIN Batch 8/200 loss 0.965834 acc 0.731441 lr 0.00001000 grad_norm 17.679173 rank 0
2025-08-11 07:33:56,211 WARNING get infinite grad_norm, check your code/data if it appears frequently
2025-08-11 07:34:18,991 DEBUG TRAIN Batch 8/300 loss 1.010641 acc 0.723307 lr 0.00001000 grad_norm 16.475437 rank 0
2025-08-11 07:35:12,740 DEBUG TRAIN Batch 8/400 loss 1.135595 acc 0.709677 lr 0.00001000 grad_norm 16.770626 rank 0
2025-08-11 07:36:10,941 DEBUG TRAIN Batch 8/500 loss 1.086368 acc 0.701367 lr 0.00001000 grad_norm 15.168963 rank 0
2025-08-11 07:37:04,145 DEBUG TRAIN Batch 8/600 loss 0.813224 acc 0.748869 lr 0.00001000 grad_norm 14.210589 rank 0
2025-08-11 07:38:03,561 DEBUG TRAIN Batch 8/700 loss 1.680825 acc 0.543434 lr 0.00001000 grad_norm 18.440254 rank 0
2025-08-11 07:38:56,685 DEBUG TRAIN Batch 8/800 loss 1.581684 acc 0.567755 lr 0.00001000 grad_norm 16.388720 rank 0
2025-08-11 07:39:51,134 DEBUG TRAIN Batch 8/900 loss 1.711819 acc 0.537463 lr 0.00001000 grad_norm 17.109304 rank 0
2025-08-11 07:40:44,899 INFO Epoch 8 Step 14000 on_batch_end False CV rank 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 07:41:40,603 DEBUG CV Batch 8/100 loss 5.454027 acc 0.174322  rank 0
2025-08-11 07:42:22,393 DEBUG CV Batch 8/200 loss 5.707376 acc 0.181925  rank 0
2025-08-11 07:42:34,679 INFO Epoch 8 Step 14000 CV info lr 1e-05 0 rank loss 5.552784946918488 acc 0.17707529025152324
2025-08-11 07:42:37,472 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/epoch_8_step_14000.pt
2025-08-11 07:42:38,532 DEBUG TRAIN Batch 8/1000 loss 1.710762 acc 0.549683 lr 0.00001000 grad_norm 15.445360 rank 0
2025-08-11 07:43:34,765 DEBUG TRAIN Batch 8/1100 loss 1.827854 acc 0.509948 lr 0.00001000 grad_norm 17.439413 rank 0
2025-08-11 07:44:31,650 DEBUG TRAIN Batch 8/1200 loss 1.450969 acc 0.600619 lr 0.00001000 grad_norm 15.116711 rank 0
2025-08-11 07:45:26,117 DEBUG TRAIN Batch 8/1300 loss 1.579979 acc 0.574173 lr 0.00001000 grad_norm 16.265848 rank 0
2025-08-11 07:46:22,024 DEBUG TRAIN Batch 8/1400 loss 1.390251 acc 0.636829 lr 0.00001000 grad_norm 18.182146 rank 0
2025-08-11 07:47:16,501 DEBUG TRAIN Batch 8/1500 loss 1.005010 acc 0.725882 lr 0.00001000 grad_norm 15.777644 rank 0
2025-08-11 07:48:14,286 DEBUG TRAIN Batch 8/1600 loss 1.914725 acc 0.490946 lr 0.00001000 grad_norm 17.487293 rank 0
2025-08-11 07:48:29,762 INFO Epoch 8 Step 14632 on_batch_end True CV rank 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 07:49:25,157 DEBUG CV Batch 8/100 loss 5.941054 acc 0.136853  rank 0
2025-08-11 07:50:07,446 DEBUG CV Batch 8/200 loss 6.416666 acc 0.149068  rank 0
2025-08-11 07:50:19,563 INFO Epoch 8 Step 14632 CV info lr 1e-05 0 rank loss 5.489966521501541 acc 0.17841639456152916
2025-08-11 07:50:22,341 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/epoch_8_whole.pt
2025-08-11 07:50:22,359 INFO Epoch 9 TRAIN info lr 1e-05 rank 0
2025-08-11 07:50:22,359 INFO using accumulate grad, new batch size is 1 times larger than before
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 07:51:30,145 DEBUG TRAIN Batch 9/100 loss 2.185543 acc 0.476954 lr 0.00001000 grad_norm 23.444582 rank 0
2025-08-11 07:52:23,292 DEBUG TRAIN Batch 9/200 loss 1.114935 acc 0.708291 lr 0.00001000 grad_norm 17.424931 rank 0
2025-08-11 07:53:20,863 DEBUG TRAIN Batch 9/300 loss 1.563303 acc 0.615690 lr 0.00001000 grad_norm 16.463619 rank 0
2025-08-11 07:53:57,958 INFO Epoch 9 Step 15000 on_batch_end False CV rank 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 07:54:53,399 DEBUG CV Batch 9/100 loss 6.190807 acc 0.144974  rank 0
2025-08-11 07:55:35,576 DEBUG CV Batch 9/200 loss 6.420388 acc 0.156218  rank 0
2025-08-11 07:55:47,188 INFO Epoch 9 Step 15000 CV info lr 1e-05 0 rank loss 6.040323257684707 acc 0.1705009612031281
2025-08-11 07:55:50,021 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/epoch_9_step_15000.pt
2025-08-11 07:56:06,863 DEBUG TRAIN Batch 9/400 loss 1.081339 acc 0.721560 lr 0.00001000 grad_norm 14.607377 rank 0
2025-08-11 07:57:03,500 DEBUG TRAIN Batch 9/500 loss 1.442833 acc 0.611389 lr 0.00001000 grad_norm 16.944687 rank 0
2025-08-11 07:57:57,338 DEBUG TRAIN Batch 9/600 loss 1.409721 acc 0.616279 lr 0.00001000 grad_norm 18.305557 rank 0
2025-08-11 07:58:57,154 DEBUG TRAIN Batch 9/700 loss 1.176376 acc 0.669371 lr 0.00001000 grad_norm 15.149367 rank 0
2025-08-11 07:59:50,118 DEBUG TRAIN Batch 9/800 loss 0.940013 acc 0.730233 lr 0.00001000 grad_norm 16.845800 rank 0
2025-08-11 08:00:44,069 DEBUG TRAIN Batch 9/900 loss 0.964052 acc 0.722345 lr 0.00001000 grad_norm 15.592501 rank 0
2025-08-11 08:01:38,977 DEBUG TRAIN Batch 9/1000 loss 0.880014 acc 0.765306 lr 0.00001000 grad_norm 18.903170 rank 0
2025-08-11 08:02:33,693 DEBUG TRAIN Batch 9/1100 loss 1.021930 acc 0.718172 lr 0.00001000 grad_norm 18.737366 rank 0
2025-08-11 08:03:30,246 DEBUG TRAIN Batch 9/1200 loss 1.520558 acc 0.580709 lr 0.00001000 grad_norm 16.115343 rank 0
2025-08-11 08:03:43,297 WARNING get infinite grad_norm, check your code/data if it appears frequently
2025-08-11 08:04:24,434 DEBUG TRAIN Batch 9/1300 loss 1.220238 acc 0.670374 lr 0.00001000 grad_norm 18.147491 rank 0
2025-08-11 08:05:02,724 INFO Epoch 9 Step 16000 on_batch_end False CV rank 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 08:05:58,157 DEBUG CV Batch 9/100 loss 6.104866 acc 0.149114  rank 0
2025-08-11 08:06:40,176 DEBUG CV Batch 9/200 loss 6.383988 acc 0.132955  rank 0
2025-08-11 08:06:52,321 INFO Epoch 9 Step 16000 CV info lr 1e-05 0 rank loss 6.087348327875137 acc 0.171102912530303
2025-08-11 08:06:55,130 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/epoch_9_step_16000.pt
2025-08-11 08:07:12,376 DEBUG TRAIN Batch 9/1400 loss 1.080184 acc 0.699010 lr 0.00001000 grad_norm 17.016106 rank 0
2025-08-11 08:08:07,082 DEBUG TRAIN Batch 9/1500 loss 0.862201 acc 0.751096 lr 0.00001000 grad_norm 14.840633 rank 0
2025-08-11 08:09:07,545 DEBUG TRAIN Batch 9/1600 loss 1.079005 acc 0.733333 lr 0.00001000 grad_norm 16.876778 rank 0
2025-08-11 08:09:20,456 INFO Epoch 9 Step 16256 on_batch_end True CV rank 0
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-08-11 08:10:15,813 DEBUG CV Batch 9/100 loss 6.203761 acc 0.149114  rank 0
2025-08-11 08:10:57,573 DEBUG CV Batch 9/200 loss 6.540901 acc 0.149639  rank 0
2025-08-11 08:11:09,296 INFO Epoch 9 Step 16256 CV info lr 1e-05 0 rank loss 6.068366794586182 acc 0.16933256256580353
2025-08-11 08:11:12,121 INFO [Rank 0] Checkpoint: save to checkpoint exp/greek_sft/epoch_9_whole.pt
2025-08-11 08:11:12,231 DEBUG Attempting to acquire lock 139905800125504 on /home/ubuntu/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-11 08:11:12,231 DEBUG Lock 139905800125504 acquired on /home/ubuntu/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-11 08:11:12,233 DEBUG Attempting to release lock 139905800125504 on /home/ubuntu/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-11 08:11:12,233 DEBUG Lock 139905800125504 released on /home/ubuntu/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
2025-08-11 08:11:12,253 DEBUG Attempting to acquire lock 139905800125504 on /home/ubuntu/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-11 08:11:12,253 DEBUG Lock 139905800125504 acquired on /home/ubuntu/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-11 08:11:12,257 DEBUG Attempting to release lock 139905800125504 on /home/ubuntu/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
2025-08-11 08:11:12,257 DEBUG Lock 139905800125504 released on /home/ubuntu/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
