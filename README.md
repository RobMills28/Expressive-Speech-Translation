# ğŸ—£ï¸ Expressive Speech Translation

**A proof-of-concept for a multimodal, identity-preserved speech translation pipeline.** This system translates spoken content from video or audio files into a new language whilst preserving the original speaker's unique vocal characteristics, emotional tone, and natural timing.

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python&logoColor=white)
![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react&logoColor=black)
![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat&logo=docker&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=pytorch&logoColor=white)
![Flask](https://img.shields.io/badge/Flask-000000?style=flat&logo=flask&logoColor=white)
![AI](https://img.shields.io/badge/AI-Speech%20Translation-FF6B6B?style=flat)

## âœ¨ Core Features

ğŸ­ **Vocal Identity Preservation** â€” Utilises zero-shot voice cloning to ensure the translated speech sounds like the original speaker

ğŸ˜Š **Emotional Congruence** â€” Maintains the emotional prosody (tone, pitch, rhythm) of the original speech in the translated output

ğŸ¬ **Audio-Visual Coherence** â€” Generates accurate lip-synchronisation for video content, matching the new translated audio track

ğŸŒ **Multilingual Support** â€” Built on a powerful NMT model capable of translating between hundreds of languages

ğŸ—ï¸ **Modular Architecture** â€” Designed as a flexible and scalable system of independent microservices

## ğŸ›ï¸ Technical Architecture

This project is built using a **Modern Cascaded Framework (MCF)**, a modular microservice architecture designed for robustness and scalability. A central **Orchestrator Service** (built with Flask) manages the workflow and communicates with a set of independent, containerised AI services.

### The pipeline consists of four primary AI models:

#### ğŸ¤ 1. Automatic Speech Recognition (ASR)
- **Model:** OpenAI Whisper
- **Purpose:** Transcribes the source audio into text with high accuracy

#### ğŸ”„ 2. Neural Machine Translation (NMT)
- **Model:** Meta NLLB (No Language Left Behind)
- **Purpose:** Translates the source text into the target language

#### ğŸ—£ï¸ 3. Expressive Text-to-Speech (TTS)
- **Model:** CosyVoice
- **Purpose:** Synthesises the translated text into speech. It uses the original source audio as a voice prompt to perform zero-shot voice cloning, preserving the speaker's unique vocal timbre

#### ğŸ‘„ 4. Lip-Synchronisation
- **Model:** MuseTalk
- **Purpose:** Takes the original video (without audio) and the newly generated translated audio, and produces a final video with perfectly synchronised lip movements

## ğŸ”„ How It Works (High-Level Process Flow)

1. ğŸ“¤ A user uploads a source video or audio file via the React frontend
2. ğŸ›ï¸ The Flask **Orchestrator** receives the file and extracts the audio
3. ğŸ¤ The audio is sent to the **ASR Service (Whisper)** to be transcribed
4. ğŸ”„ The resulting source text is sent to the **NMT Service (NLLB)** to be translated
5. ğŸ—£ï¸ The translated text, along with the original source audio (as a voice reference), is sent to the **Expressive TTS Service (CosyVoice)**. This service generates the final translated audio in the original speaker's voice
6. ğŸ¬ For video, the original video frames and the new translated audio are sent to the **Lip-Sync Service (MuseTalk)**, which generates the final, perfectly synchronised video

## ğŸ› ï¸ Tech Stack

**Backend:**
- ğŸ Python
- âš¡ Flask
- ğŸ³ Docker
- ğŸ¦„ Gunicorn

**Frontend:**
- âš›ï¸ React
- ğŸ“ JavaScript
- ğŸ¨ Tailwind CSS
- ğŸ­ Shadcn/UI
- ğŸŒŠ Wavesurfer.js

**AI Models:**
- ğŸ”¥ PyTorch
- ğŸ¤— Transformers
- ğŸ¤ Whisper
- ğŸŒ NLLB
- ğŸ—£ï¸ CosyVoice
- ğŸ‘„ MuseTalk

**DevOps/Deployment:**
- ğŸ³ Docker Compose for local orchestration
- â˜ï¸ Designed for cloud infrastructure (AWS) or local HPC cluster with Slurm

## ğŸš€ Running the Project

This project is composed of a frontend application and several backend microservices.

### ğŸ”§ Backend Setup

1. Navigate to the Backend directory
2. Create a Python virtual environment (e.g., with Conda) and install the dependencies from `requirements.txt`
3. The TTS and Lip-Sync models are run as separate Docker containers. Navigate to the Docker directory and use docker-compose to build and run the services:

```bash
docker-compose up -d cosyvoice musetalk
```

4. Start the main Flask application:

```bash
python app.py
```

### ğŸ¨ Frontend Setup

1. Navigate to the Frontend directory
2. Install dependencies:

```bash
npm install
```

3. Start the development server:

```bash
npm start
```

4. Open `http://localhost:3000` in your web browser

## ğŸ“ Project Structure

```
expressive-speech-translation/
â”œâ”€â”€ Backend/               # Flask orchestrator and API endpoints
â”œâ”€â”€ Frontend/              # React user interface
â”œâ”€â”€ Docker/                # Container configurations for AI services
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â””â”€â”€ docker-compose.yml    # Service orchestration
```

## ğŸ¤ Contributing

This is a research prototype and proof-of-concept. For academic or research collaboration enquiries, please open an issue or contact the maintainer.

## ğŸ“„ Licence

This project is currently under development. Please contact the author for usage permissions and licensing enquiries.

## ğŸ‘¨â€ğŸ’» Author

**Robert Mills** - [RobMills28](https://github.com/RobMills28)

---

*An advanced speech translation system demonstrating the potential for preserving human expressiveness in cross-lingual communication.*
